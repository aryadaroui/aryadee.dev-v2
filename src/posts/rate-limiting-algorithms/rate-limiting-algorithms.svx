---
title: "Rate Limiting Algorithms Compared "
date: 03 July 2023
update:
thumbnail: ./thumbnail.png
tags: 
  - webdev
  - Python
  - code
---

<script>
  import {math, display} from 'mathlifier';
  import ExpandingBox from '$lib/ExpandingBox.svelte';
  import fixed_window from './fixed_window.json'
  import fixed_window_cross_window from './fixed_window_cross_window.json'
  import enforced_avg from './enforced_avg.json'
  import sliding_window from './sliding_window.json'
  import leaky_bucket_soft from './leaky_bucket_soft.json'
  import leaky_bucket_hard from './leaky_bucket_hard.json'
  import subplots from './subplots.json'
</script>

# Intro

When making any sort of API available on the web, one of the first concerns you run into is:

> How do I protect an endpoint from being inundated by requests so that I can maintain system stability?

Assuming you already have [auth](https://en.wikipedia.org/wiki/Authorization) figured out, the go-to solution is limiting the rate (rate limiting) of processed requests--either by user, or globally on the endpoint.

There are a handful of algorithms used for rate limiting. I'm going to go over how they each work, and provide some code samples and plots along with it. The algorithms I'm going to cover are:

1. Fixed window
2. Enforced average
3. Sliding window
4. Leaky bucket (Generic Cell Rate)

You can find the full Python code [on my GitHub](https://github.com/aryadaroui/rate-limiting-algorithms).

<Callout title="Architecture abstraction" icon="smile" color='pink'>
Distributed, scalable systems commonly use an external cache to store rate limiting data. This is so that the rate limit can be enforced across multiple instances of the application.

In my code samples, I'm using an abstraction of a Redis-like cache data store. If you use my code samples, you'll need to replace my `cache` with your own implementation.
</Callout>

## Problem setup

So that we have a clear definition for the problem we're trying to solve:

>  In a system that is receiving requests at $r$ requests per second, identify which requests to accept to maintain a limit of $l$ accepted requests per time $w$, for a rate limit of $l_\text{rps}$.

For our examples, suppose we have a system that receives 10 requests per second, and we want to limit it to 5 requests per second.

$$
\begin{aligned}
    r &= 10 \text{ req/s}\\
    l &= 5 \text{ req} \\
    w &= 1 \text{ s} \\
    l_\text{rps} &= {l \over w} \\
    &= 5 \text{ req/s}
  \end{aligned}
$$

# Algorithms

## Fixed window

The fixed window algorithm is straightforward. When we receive the first request, we start a timer with a duration of $w$ and increment a `counter` in our cache. For each subsequent request within the window, we increment the counter until we hit the limit, after which we stop accepting requests. It's important to note that the `incr(){:python}` command does not reset the [TTL](#glossary) of the cache entry.

```python
def fixed_window(key: str, limit: float, window_length_ms: float) -> dict

	counter = cache.get(key)

	if counter is not None:  # cache entry exists
		if count < limit:
      cache.incr(key) # incr() does not reset TTL
			return {"status": 'OK', "counter": counter + 1}

		else:  # we hit counter limit
			return {"status": "DENIED", "counter": counter}

	else:  # cache entry does not exist
		cache.set(key, 1, window_length_ms)  # set the target cache entry with TTL of window_length_ms
		return {"status": "OK", "counter": 1}
```

<ExpandingBox>
  <Plot plot_json={fixed_window} />
</ExpandingBox>

Looking at the plot, we see that we're effective in limiting requests arriving at a uniform rate, but there there are two things two point out. First, we see that the requests are accepted greedily; there is no minimum interval between accepted requests. Second--and this isn't as obvious--it's possible we can **overshoot** the limit across window boundaries.

### Cross-window flaw

Because the fixed window models the continuous flow of time into, well, fixed windows, at every window boundary, the `counter` resets, erasing the true number of requests in the last $w$ seconds. So a burst of requests arriving at the end of a window can be accepted, even if it puts us over the limit.

<ExpandingBox>
  <Plot plot_json={fixed_window_cross_window} />
</ExpandingBox>

To account for this, we can calculate the maximum cross-window overshoot:

$$
\text{overshoot}_\text{max} = l-1 \\
$$

But, keep in mind that the overshoot is relative to the limit,

$$
l_\text{total} = l + \text{overshoot}_\text{max} = 2l-1
$$

So for $l=5$, we could hit have up to  $l_\text{total}=9$ accepted requests across window boundaries.


<Callout title='Using a static timer' icon="warning" color='orange'>

The $-1$ is because we start our window upon first request, which always excludes the first request from requests in the following window.
But, if your implementation uses a static timer (e.g. every UTC second), all requests can be included in the first window, so,

$$
\text{overshoot}_\text{max} = l\\
l_\text{total} = l + \text{overshoot}_\text{max} = 2l
$$

</Callout>

<!-- <h2 id="enforced-average">Enforced average</h2> -->

## Enforced average

The enforced average rate limiting algorithm is a method that regulates the time between requests instead of the rate itself. Typically, rate limiting is  measured in $\text{req} / \text{s}$, but in this algorithm, we focus on the reciprocal $\text{s} / \text{req}$, which represents the average time between requests if we evenly distribute $l$ of requests over a duration $w$.

To enforce this average, we introduce a waiting time $\tau$, which specifies the number of seconds we should wait between accepting  requests.
$$
\begin{aligned}
l_\text{rps} &= 5 \text{ req/s} = \frac{1}{\tau_\text{spr}} \\
\implies \tau_\text{spr} &= \frac{1 \text{ s}}{5 \text{ req}} = 0.2 \text{ s/req} \\
\implies \tau &= 0.2 \text{ s}
\end{aligned}
$$

In this algorithm, we set $\tau$ as the expiry timer of our cache entry. If the entry exists, we've already received a request in $\tau$ seconds, and we deny it; otherwise, we accept it.

```python
def enforced_average(key: str, limit: float, window_length_ms: float) -> dict:
	avg_time_ms = (window_length_ms / limit)

	entry = cache.get(key)

	if entry is not None:  # cache entry exists
		return {"status": "DENIED"}

	else:  # cache entry does not exist
		cache.set(key, 1, avg_time_ms)  # set cache entry with TTL of avg_time_ms
		return {"status": "OK"}
```

<ExpandingBox>
  <Plot plot_json={enforced_avg} />
</ExpandingBox>

The key feature of this limiter is that it has a minimum interval of $\tau$ seconds between accepted requests, yielding a more even distribution.

## Sliding window

The sliding window rate limiting algorithm addresses the window boundary issue present in the fixed window algorithm. Instead of using a dedicated counter, this algorithm records the timestamp of every request and counts the number of timestamps within the last $w$ seconds. If the count is less than the limit, the request is accepted. Additionally, an expiry timer is (re)set for $w$ seconds upon receiving each request to keep the cache free from stale entries.

```python
def sliding_window(key: str, limit: float, window_length_ms: float = 1000):

	times: list = cache.get(key)
	if times is not None:  # cache entry exists

		# remove all times that are outside the window
		times = [time for time in times if globals.CURRENT_TIME - time < window_length_ms]

		if len(times) < limit:
			times.append(globals.CURRENT_TIME)
			cache.set(key, times, window_length_ms)
			return {"status": "OK", "counter": len(times), "new": False}

		else:
			return {"status": "DENIED", "counter": len(times), "new": False}

	else:
		cache.set(key, [globals.CURRENT_TIME], window_length_ms)
		return {"status": "OK", "counter": 1, "new": True}
```

<Callout title='Leveraging Redis' icon="lightning" color='yellow'>

I kept the code example generic, but we could offload more of the work to Redis and get the count directly from the cache.

Instead of storing the timestamps in a list, we could use a [sorted set](https://redis.io/docs/data-types/sorted-sets/) with the timestamp as the score. Using [`ZREMRANGEBYSCORE`](https://redis.io/commands/zremrangebyscore), we remove all timestamps older than $w$ seconds and get the count of what's left with [`ZCOUNT`](https://redis.io/commands/zcount/).

</Callout>

<ExpandingBox>
  <Plot plot_json={sliding_window} />
</ExpandingBox>

This is a better model of continuous time compared to the fixed window, but it comes at the cost of having to record and manage up to $l$-many timestamps for every request, whereas in other algorithms, we only need to increment a counter. If you have a high $l$, this can be a nontrivial cost.

## Leaky bucket

For the leaky bucket algorithm, we accept requests until our bucket is full, waiting for it to leak more capacity. Normally, we'd have parameters of `bucket_size` and `leak_rate`, which ties in the analogy--but I don't find these intuitive for rate limiting. Instead, I'm going to keep the same `limit` and `window_length_ms` parameters from the other algorithms, with an additional `mode` parameter for `'soft'` or `'hard'` limiting that I'll explain shortly.

When we receive our first request, we start a counter in our cache entry and give it a timestamp. For each subsequent request, we calculate what our counter decreases (leaks) to from the last timestamp; if's under the limit, we accept it. Like in the sliding window, to keep our cache tidy, we set the expiry timer on every request to be the time it would take for the counter to leak to 0.

The "soft" and "hard" modes set the leak rate to $l/w$ and $1/w$, respectively. This sets the transient and steady state limiting behavior, as shown in the table below.

<ExpandingBox>

| Mode  | Transient overshoot max | Steady-state avg |
| :---- | :---------------------: | :--------------------: |
| soft  |  $l-1$                  |  $l/w$               |
| hard  |  $0$                    |  $1 \text{ req} / w$ |

</ExpandingBox>

This code block is a bit longer than our previous ones, but it's mostly bloat from error checking and packing. The core logic is fairly similar to before.

```python
def leaky_bucket(key: str, limit: float, window_length_ms: float = 1000, mode = 'soft') -> dict:

	if mode == 'soft':
		leak_rate = limit # leak at limit-many requests per window
	elif mode == 'hard':
		leak_rate = 1 # leak at 1 request per window
	else:
		raise ValueError(f'Invalid mode: {mode}')

	entry: dict = cache.get(key)

	if entry is not None:  # cache entry exists
		# unpack the cache entry because `['key']` notation is ugly
		counter = entry['counter']
		time = entry['time']

		delta_time_ms = (globals.CURRENT_TIME - time)  # time since last request
		counter = max(counter - (delta_time_ms * leak_rate) / window_length_ms, 0) # get the extrapolated "leaked" counter value

		if counter + 1 < limit:  # increment the counter
			cache.set(
				key,
        {
					'counter': counter + 1,
					'time': globals.CURRENT_TIME
				},
        (counter + 1) * 1000 / leak_rate # this is the TTL
			)
			return {"status": "OK", "counter": counter + 1, "new": False}

		else:  # we hit counter threshold
			return {"status": "DENIED", "counter": counter, "new": False}

	else:  # cache entry does not exist
		cache.set(
			key,
      {
				'counter': 1,
				'time': globals.CURRENT_TIME
			},
      window_length_ms / leak_rate # this is the TTL
		) 
		return {"status": "OK", "counter": 1, "new": True}
```

<ExpandingBox>
  <Plot plot_json={leaky_bucket_soft} />
</ExpandingBox>

<ExpandingBox>
  <Plot plot_json={leaky_bucket_hard} />
</ExpandingBox>

Our leaky bucket is like a blend of all our previous algorithms. It is continuous-time like the sliding window, manages only a single counter like the fixed window, can over shoot on transients like the fixed window, and enforces a minimum acceptance interval at steady state like the enforced average

# Conclusion

In summary:

- **Fixed window**
  - Greedy. No min interval between accepted requests
  - Cross-window flaw
- **Enforced average**
  - Strict. Has minimum interval between accepted requests
- **Sliding window**
  - Expensive, but better version of fixed window
  - Greedy. No min interval between accepted requests
- **Leaky bucket**
  - No min interval on transient, but has min interval at steady state
  - "Soft" mode overshoots on transients
  - "Hard" mode has low steady state average

This is shown in more detail in the table below.

<ExpandingBox center='false'>

| Algorithm                        |   Overshoot max    |     Steady-state avg   |       Stored data      |     Interval min     |
| :------------------------------- | :----------------: | :--------------------: | :--------------------: | :------------------: |
| `fixed_window(...)`              | $l-1$ cross-window |         $l/w$          |        `counter`       |          0           |
| `enforced_avg(...)`              |         0          |         $l/w$          |           `1`          |        $w/l$         |
| `sliding_window(...)`            |         0          |         $l/w$          | $l$-many `timestamps`  |          0           |
| `leaky_bucket(..., mode='soft')` |  $l-1$ transient   |         $l/w$          | `counter`, `timestamp` |    0 T, $w/l$ SS     |
| `leaky_bucket(..., mode='hard')` |         0          | ${\color{magenta}1}/w$ | `counter`, `timestamp` |    0 T, $1/w$ SS     |

</ExpandingBox>

# Appendix


<Callout title='Other features to consider' icon="smile">

There are other features you may want to explore that I did not cover here, such as:

- Traffic shaping
  - You can place your denied requests into a queue until they're ready to be processed, instead of rejecting them outright
  - Also known as *traffic shaping* vs. *traffic policing*

- Timeout
  - Block requests for extra time if there are way too many requests
  - Effectively a two-stage limiter

- No relaxation
  - Increment the `counter` even if the request is denied.
  - Similar, simpler effect to timeout

- Latency compensation
  - Compensate for the time it takes when getting and setting
  - I think this is pretty niche. Only needed for high frequency systems

</Callout>

<Callout title='Leaky bucket terminology' icon="info">

There is reasonable confusion over the terminology of leaky bucket, token bucket, and Generic Cell Rate Algorithm (GCRA) on the internet. 

- In a **leaky bucket**...
  - Each request fills up the bucket by a constant amount
  - When the bucket is full, the request is invalid
  - The bucket leaks at a constant rate

- In a **token bucket**...
  - There are request tokens in the bucket, and each request consumes a token (leaks the bucket)
  - When the bucket is empty, the request is invalid
  - The bucket is refilled with tokens at a constant rate

- Both leaky and token bucket are used to shape network traffic by using the bucket "*as a queue*", but they can also be used "*as a meter*" for rate limiting by using the bucket as a counter and incrementing/decrementing it at a constant rate
  - Classically, "constant rate" is done by a literal background process that manages the leak/refill of the bucket

- The **GCRA** is the same as the *leaky bucket as a meter*, but by knowing the leak rate and keeping track of the last timestamp, we can calculate how much the bucket would've leaked since then, eliminating the need for a background process to constantly leak the bucket

- The leaky bucket and token bucket are effectively the same "as a meter", but there are differences as a queue; you can read more about it on [wikipedia](https://en.wikipedia.org/wiki/Leaky_bucket)

For this article, I just use the term "leaky bucket", because the GCRA optimizization is not worth distinguishing.

</Callout>

<Callout title='Reading the plots' icon="info">

- Traces
  - *OK* and *DENIED* are incoming requests that were accepted or denied, respectively
  - *counter* is the algorithm's internal `counter`. The "#" is for the lifetime of the cache entry
  - *num OKs* is the number of requests accepted in the last $w$ seconds

- Reference lines
  - *limit* is the given limit
  - The <span style="color: forestgreen;">green</span> line is the start of a new cache entry
  - The <span style="color: firebrick;">red</span> line is the expiration of a cache entry
  - The <span style="color: goldenrod;">yellow</span> line is the overlap of start and expiration lines

</Callout>

<Callout title='Glossary' icon="info">

<ExpandingBox center=false>

<div id='glossary'> </div>

| Term          | Definition                                                                                     |
|:---------------|:------------------------------------------------------------------------------------------------|
| **cache**         | Data store optimized for fast reads. Usually a key-value store.|
| **Greedy**        | Accepts requests as fast as possible. No min interval between accepted requests.|
| **Interval**      | The time between accepted requests.|
| **key**           | Unique identifier for the cache entry. This is usually the IP address of the requester.|
| **Overshoot**     | The number of requests that exceed the given limit                                             |
| **Requests**      | Incoming API requests                                                                         |
| **rps**           | Requests per second                                                                            |
| **Steady-state**  | The long-term effect of a system after it settles. For leaky bucket, this is the bucket leaking at a constant rate|
| **SS**            | Shorthand for "steady state"                                                                   |
| **T**             | Shorthand for "transient"                                                                      |
| **TTL**           | Time-to-live. The remaining time until the cache entry expires                                 |
| **Transient**     | The initial effect of a system, before it reaches steady state. For leaky bucket, this is the initial filling of the bucket |

</ExpandingBox>

</Callout>
