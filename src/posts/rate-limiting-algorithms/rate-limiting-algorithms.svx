---
title: "Rate Limiting Algorithms Compared "
date: 03 July 2023
update:
thumbnail: ./thumbnail.png
tags: 
  - webdev
  - TypeScript
  - Python
  - code
---

<script>
  import {math, display} from 'mathlifier';
  import ExpandingBox from '$lib/ExpandingBox.svelte';
  import fixed_window from './fixed_window.json'
  import fixed_window_cross_window from './fixed_window_cross_window.json'
  import enforced_avg from './enforced_avg.json'
  import sliding_window from './sliding_window.json'
  import leaky_bucket_soft from './leaky_bucket_soft.json'
  import leaky_bucket_hard from './leaky_bucket_hard.json'
  import subplots from './subplots.json'
</script>

# Intro

When making any sort of API available on the web, one of the first concerns you run into is:

> How do I protect an endpoint from being inundated by requests, so that I can maintain system stability?

Assuming you already have [auth](https://en.wikipedia.org/wiki/Authorization) figured out, the go-to solution is limiting the rate (rate limiting) of processed requests--either by user, or globally on the endpoint.

There are a handful of algorithms used for rate limiting. I'm going to go cover how they each work, and provide some code samples and plots along with it. You can find the full code, with proper Python dataclasses (and TypeScript interfaces) [on my GitHub](www.github.com/aryadaroui/rate-limiting-algorithms). The algorithms we're going to cover are:

1. Fixed window
2. Enforced average
3. Sliding window
4. Leaky bucket (Generic Cell Rate)

<Callout title="Architecture abstraction" icon="smile" color='pink'>
Distributed, scalable systems commonly use an external cache to store rate limiting data. This is so that the rate limit can be enforced across multiple instances of the application.

In my code samples, I'm using an abstraction of a Redis-like cache data store. If you use my code samples, you'll need to replace my `cache` with your own implementation.
</Callout>

## Problem setup

So that we have a clear definition for the problem we're trying to solve:

>  In a system that is receiving requests at $r$ requests per second, identify which requests to accept to maintain a limit of $l$ accepted requests per time $w$, for a rate limit of $l_\text{rps}$.

For our examples, suppose we have a system that receives 10 requests per second, and we want to limit it to 5 requests per second.

$$
\begin{aligned}
    r &= 10 \text{ req/s}\\
    l &= 5 \text{ req} \\
    w &= 1 \text{ s} \\
    l_\text{rps} &= {l \over w} \\
    &= 5 \text{ req/s}
  \end{aligned}
$$

# Algorithms

## Fixed window

Probably the most intuitive rate limiting algorithms is the fixed window. In this algorithm, we set a `counter` in our cache with a $w$-length expiration timer when we receive the first request. For every subsequent request within the window, we increment the count. When the counter goes over the limit, we stop accepting requests. Note that the `incr(){:python}` command does not reset the [TTL](#ttl) of the cache entry.

```python
def fixed_window(key: str, limit: float, window_length_ms: float) -> dict

	counter = cache.get(key)

	if counter is not None:  # cache entry exists
		if count < limit:
      cache.incr(key) # incr() does not reset TTL
			return {"status": 'OK', "counter": counter + 1}

		else:  # we hit counter limit
			return {"status": "DENIED", "counter": counter}

	else:  # cache entry does not exist
		cache.set(key, 1, window_length_ms)  # set the target cache entry with TTL of window_length_ms
		return {"status": "OK", "counter": 1}
```

<ExpandingBox>
  <Plot plot_json={fixed_window} />
</ExpandingBox>

Looking at the plot, we see that we're effective in limiting requests arriving at a uniform rate, but there there are two things two point out. First, we see that the requests are accepted greedily; there is no minimum interval between accepted requests. Second--and this isn't as see to see--it's possible we can **overshoot** the limit across window boundaries.

### Cross-window flaw

Because fixed window inherently models the continuous flow of time into fixed windows, at every window boundary, the `counter` resets, losing track of the true number of requests in the last $w$ seconds. So a burst of requests arriving at the end of a window can be accepted, even if it puts us over the limit.

<ExpandingBox>
  <Plot plot_json={fixed_window_cross_window} />
</ExpandingBox>

To account for this, we can calculate the maximum cross-window overshoot:

$$
\text{overshoot}_\text{max} = l-1 \\
$$
 
But, keep in mind that the overshoot is relative to the limit,

$$
l_\text{total} = l + \text{overshoot}_\text{max} = 2l-1
$$

So for $l=5$, we could hit have up to  $l_\text{total}=9$ accepted requests across window boundaries.


<Callout title='Using a static timer' icon="warning" color='orange'>

The $-1$ is because we start our window upon first request, which always excludes the first request from requests in the following window.
But, if your implementation uses a static timer (e.g. every UTC second), all requests can be included in the first window, so,

$$
\text{overshoot}_\text{max} = l\\
l_\text{total} = l + \text{overshoot}_\text{max} = 2l
$$

</Callout>

<!-- <h2 id="enforced-average">Enforced average</h2> -->
## Enforced average

The enforced average rate limiting algorithm focuses on the average time between requests rather than the rate itself. Normally, we think about rate limiting in units $\text{req} / \text{s}$, but we can also describe it by its reciprocal: $\text{s} / \text{req}$. This reciprocal represents the average time between requests if we were to distribute $l$ requests evenly across $w$ seconds. 


We can enforce this average by waiting $\tau$ seconds between accepted requests, where $\tau$ is our *waiting time*.

$$
\begin{aligned}
l_\text{rps} &= 5 \text{ req/s} = \frac{1}{\tau_\text{spr}} \\
\implies \tau_\text{spr} &= \frac{1 \text{ s}}{5 \text{ req}} = 0.2 \text{ s/req} \\
\implies \tau &= 0.2 \text{ s}
\end{aligned}
$$

In this algorithm, we set $\tau$ as the expiry timer of our cache entry. If the entry exists, we've already received a request in $\tau$ seconds, and we deny it; otherwise, we accept it.

```python
def enforced_average(key: str, limit: float, window_length_ms: float) -> dict:
	avg_time_ms = (window_length_ms / limit)

	entry = cache.get(key)

	if entry is not None:  # cache entry exists
		return {"status": "DENIED"}

	else:  # cache entry does not exist
		cache.set(key, 1, avg_time_ms)  # set cache entry with TTL of avg_time_ms
		return {"status": "OK"}
```

<ExpandingBox>
  <Plot plot_json={enforced_avg} />
</ExpandingBox>

The key feature of this limiter is that it has a minimum interval of $\tau$ seconds between accepted requests, yielding a more even distribution.

## Sliding window

The sliding window rate limiting algorithm addresses the window boundary issue present in the fixed window algorithm. Instead of using a dedicated counter, this algorithm records the timestamp of every request and counts the number of timestamps within the last $w$ seconds. If the count is less than the limit, the request is accepted; otherwise, it is denied. Additionally, an expiry timer is set for $w$ seconds upon receiving a request to keep the cache free from stale entries.

```python
def sliding_window(key: str, limit: float, window_length_ms: float = 1000):

	times: list = cache.get(key)
	if times is not None:  # cache entry exists

		# remove all times that are outside the window
		times = [time for time in times if globals.CURRENT_TIME - time < window_length_ms]

		if len(times) < limit:
			times.append(globals.CURRENT_TIME)
			cache.set(key, times, window_length_ms)
			return {"status": "OK", "counter": len(times), "new": False}

		else:
			return {"status": "DENIED", "counter": len(times), "new": False}

	else:
		cache.set(key, [globals.CURRENT_TIME], window_length_ms)
		return {"status": "OK", "counter": 1, "new": True}
```

<Callout title='Leveraging Redis' icon="lightning" color='blue'>

I kept the code example generic, but we could offload more of the work to Redis and get the count directly from the cache.

Instead of storing the timestamps in a list, we could use a [sorted set](https://redis.io/docs/data-types/sorted-sets/) with the timestamp as the score. Using [`ZREMRANGEBYSCORE`](https://redis.io/commands/zremrangebyscore), we remove all timestamps older than $w$ seconds and get the count of what's left (within the window) with [`ZCOUNT`](https://redis.io/commands/zcount/).

</Callout>

<ExpandingBox>
  <Plot plot_json={sliding_window} />
</ExpandingBox>

This is a better model of continuous state compared to the fixed window, but it comes at the cost of having to record and manage sets of timestamps for every request, whereas in other algorithms, we only need to increment a counter. If you have a high $l$, this can be a nontrivial cost.

## Leaky bucket

For the leaky bucket algorithm, we accept requests until our bucket is full, waiting for it to leak more capacity. Normally, we'd have parameters of `bucket_size` and `leak_rate`, which ties in the analogy, but I don't find it intuitive for rate limiting. Instead, I'm going to keep the same `limit` and `window_length_ms` parameters from the other algorithms, with an additional `mode` parameter for `'soft'` or `'hard'` limiting, which determines the leak rate.

Our leaky bucket is like a blend of all of our previous algorithms. When we receive our first request, we start a counter in our cache entry and give it a timestamp. For each subsequent request, we calculate what our counter decreases (leaks) to from the last timestamp and add one for the new request; if it goes over the limit, we deny it. Additionally, to keep our cache tidy, we set the expiry timer on every request to be the time it would take for the counter to leak to 0.

The soft and hard modes set the leak rate to $l/w$ and $1/w$, respectively. This sets the transient and steady-state limiting behavior, as shown in the table below.

<ExpandingBox>

| Mode  | Transient overshoot max | Steady-state avg |
| :---- | :---------------------: | :--------------------: |
| soft  |  $l-1$                  |  $l/w$               |
| hard  |  $0$                    |  $1 \text{ req} / w$ |

</ExpandingBox>

This code block is a bit longer than our previous ones, but it's mostly bloat from error checking and packing. The core logic is fairly similar to before.

```python
def leaky_bucket(key: str, limit: float, window_length_ms: float = 1000, mode = 'soft') -> dict:

	if mode == 'soft':
		leak_rate = limit # leak at limit-many requests per window
	elif mode == 'hard':
		leak_rate = 1 # leak at 1 request per window
	else:
		raise ValueError(f'Invalid mode: {mode}')

	entry: dict = cache.get(key)

	if entry is not None:  # cache entry exists
		# unpack the cache entry because `['key']` notation is ugly
		counter = entry['counter']
		time = entry['time']

		delta_time_ms = (globals.CURRENT_TIME - time)  # time since last request
		counter = max(counter - (delta_time_ms * leak_rate) / window_length_ms, 0) # get the extrapolated counter value

		if counter + 1 < limit:  # increment the counter
			cache.set(
				key,
        {
					'counter': counter + 1,
					'time': globals.CURRENT_TIME
				},
        (counter + 1) * 1000 / leak_rate # this is the TTL
			)
			return {"status": "OK", "counter": counter + 1, "new": False}

		else:  # we hit counter threshold
			return {"status": "DENIED", "counter": counter, "new": False}

	else:  # cache entry does not exist
		cache.set(
			key,
      {
				'counter': 1,
				'time': globals.CURRENT_TIME
			},
      window_length_ms / leak_rate # this is the TTL
		) 
		return {"status": "OK", "counter": 1, "new": True}
```

<ExpandingBox>
  <Plot plot_json={leaky_bucket_soft} />
</ExpandingBox>

<ExpandingBox>
  <Plot plot_json={leaky_bucket_hard} />
</ExpandingBox>

The defining feature of the leaky bucket is its dynamic nature. It can handle a transient burst of requests, but also eases into an even interval of accepted requests at steady-state.

# Conclusion

In summary:

- **Fixed window**
  - Cheap
  - Greedy. No min interval between accepted requests
  - Cross-window flaw

- **Enforced average**
  - Cheap
  - Strict. Has minimum interval between accepted requests

- **Sliding window**
  - Expensive, but better version of fixed window
  - Greedy. No min interval between accepted requests
  - Resolves the cross-window flaw

- **Leaky bucket**
  - Cheap
  - Dynamic. No interval min on transient, but has min interval on steady-state
  - Soft mode overshoots on transients, which may be desirable

This is shown in more detail in the table below.

<ExpandingBox>

| Algorithm                        |   Overshoot max    |     Steady-state avg   |       Stored data      |     Interval min     |
| :------------------------------- | :----------------: | :--------------------: | :--------------------: | :------------------: |
| `fixed_window(...)`              | $l-1$ cross-window |         $l/w$          |        `counter`       |          0           |
| `enforced_avg(...)`              |         0          |         $l/w$          |           `1`          |        $w/l$         |
| `sliding_window(...)`            |         0          |         $l/w$          | $l$-many `timestamps`  |          0           |
| `leaky_bucket(..., mode='soft')` |  $l-1$ transient   |         $l/w$          | `counter`, `timestamp` |    0 T, $w/l$ SS     |
| `leaky_bucket(..., mode='hard')` |         0          | ${\color{magenta}1}/w$ | `counter`, `timestamp` |    0 T, $1/w$ SS     |

</ExpandingBox>

# Appendix

## Other features to consider

There are other features you may want to explore that I did not cover here, such as:

- Traffic shaping
  - You can place your denied requests into a queue until they're ready to be processed, instead of rejecting them outright
  - Also known as *traffic shaping* vs. *traffic policing*

- Timeout
  - Block requests for extra time if there are way too many requests
  - Effectively a two-stage limiter

- No relaxation
  - Increment the `counter` even if the request is denied.
  - Similar, simpler effect to timeout

- Latency compensation
  - Compensate for the time it takes when getting and setting
  - I think this is pretty niche. Only needed for high frequency systems

  ## Leaky bucket terminology

There is reasonable confusion over the terminology of leaky bucket, token bucket, and Generic Cell Rate Algorithm (GCRA) on the internet. 

- In a **leaky bucket**...
  - Each request fills up the bucket by a constant amount
  - When the bucket is full, the request is invalid
  - The bucket leaks at a constant rate

- In a **token bucket**...
  - There are request tokens in the bucket, and each request consumes a token (leaks the bucket)
  - When the bucket is empty, the request is invalid
  - The bucket is refilled with tokens at a constant rate

- Both leaky and token bucket are used to shape network traffic by using the bucket "*as a queue*", but they can also be used "*as a meter*" for rate limiting by using the bucket as a counter and incrementing/decrementing it at a constant rate
  - Classically, "constant rate" is done by a literal background process that manages the leak/refill of the bucket

- The **GCRA** is the same as the *leaky bucket as a meter*, but by knowing the leak rate and keeping track of the last timestamp, we can calculate how much the bucket would've leaked since then, eliminating the need for a background process to constantly leak the bucket

- The leaky bucket and token bucket are effectively the same "as a meter", but there are differences as a queue; you can read more about it on [wikipedia](https://en.wikipedia.org/wiki/Leaky_bucket)

For this article, I just use the term "leaky bucket", because the GCRA optimizization is not worth distinguishing.

## Reading the plots

- Traces
  - *OK* and *DENIED* are incoming requests that were accepted or denied, respectively
  - *counter* is the algorithm's internal `counter`. The "#" is for the lifetime of the cache entry
  - *num OKs* is the number of requests accepted in the last $w$ seconds

- Reference lines
  - *limit* is the given limit
  - The <span style="color: forestgreen;">green</span> line is the start of a new cache entry
  - The <span style="color: firebrick;">red</span> line is the expiration of a cache entry
  - The <span style="color: goldenrod;">yellow</span> line is the overlap of start and expiration lines

## Glossary

- `cache`: Data store optimized for fast reads
  - Typically in-memory, and non persistent
  - This is usually a key-value store like Redis or Memcached

- **Greedy**: accepts requests as fast as possible
  - No minimum interval between accepted requests

- **Interval**: the time between accepted requests

- `key`: unique identifier for the cache entry
  - This is usually the IP address of the requester.
  - If you're limiting the endpoint globally, it could be a static value like `'global'`

- **Overshoot**: the number of requests that exceed the given limit
  - This is usually a transient effect; does not happen at steady-state

- **Requests**: Incoming API requests
  - You could also imagine them as general, incoming network traffic

- `rps`: requests per second

- **Steady-state**: the long-term effect of a system after it settles
  - For leaky bucket, this is the bucket leaking at a constant rate

- **SS**: shorthand for "steady-state"

- **T**: shorthand for "transient"

<div id='ttl'> </div>

- **TTL**: time-to-live
  - The remaining time until the cache entry expires

- **Transient**: the initial effect of a system, before it reaches steady-state
  - For leaky bucket, this is the initial filling of the bucket

